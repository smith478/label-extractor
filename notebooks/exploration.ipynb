{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c2ce46-786c-4413-b762-33b5f00df34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d720ff7e-fc49-4537-9e32-ee29987576d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/ReportsDATASET.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f41ac53-50d4-4c86-9a57-ef50b7e3104d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad68df1d-c6cb-49e3-89fd-2257e4fad602",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Text'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de743e7f-b18e-44ab-8c4e-d6d9b5748969",
   "metadata": {},
   "source": [
    "## Run llama 3 locally\n",
    "\n",
    "- install `ollama`\n",
    "- run `ollama pull llama3` to pull down the llama 3 8B model \n",
    "- start the model running using `ollama run llama3`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e51dcb6-8346-471d-9a5b-02abc26e2c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b121a070-9d32-4ac0-b957-261ceadfc46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"http://localhost:11434/api/chat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f8cf18-804a-4ecb-b3f5-669cf3158619",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llama3(prompt: str) -> str:\n",
    "    data = {\n",
    "        \"model\": \"llama3\",\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt\n",
    "            }\n",
    "        ],\n",
    "        \"stream\": False\n",
    "    }\n",
    "    \n",
    "    headers = {\n",
    "        'Content-Type': 'application/json'\n",
    "    }\n",
    "    \n",
    "    response = requests.post(url, headers=headers, json=data)\n",
    "    \n",
    "    return response.json()['message']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e51af8-456a-4b77-a0cd-2a46a41bfb47",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = llama3(\"who wrote the book the godfather?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d4f297-b911-469a-bdbb-21a17147e9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28517819-f505-40bf-9fb8-24cd3cf2a48d",
   "metadata": {},
   "source": [
    "### Extract labels using llama 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c933ec-5b2b-4d7f-afbe-86525fe1a5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0856a2-778c-417e-a691-53d53764f151",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the list of abnormalities\n",
    "abnormalities = [\"pulmonary edema\", \"consolidation\", \"pleural effusion\", \"pneumothorax\", \"cardiomegaly\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5675f4-2c43-47c2-bc28-f342cc340a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_abnormalities(report):\n",
    "    global abnormalities\n",
    "    \n",
    "    # Initialize results\n",
    "    results = {abnormality: 0 for abnormality in abnormalities}\n",
    "    \n",
    "    # Prepare the prompt for the GPT-4 model\n",
    "    prompt = f\"Read the following radiology report and identify the presence or absence of the following abnormalities: {', '.join(abnormalities)}.\\n\\nReport:\\n{report}\\n\\nOutput the results, formatted in xml, with each of the abnormalities with 0 for absence and 1 for presence. The output should be xml with no other text.\"\n",
    "    \n",
    "    # Get the classification results from llama 3\n",
    "    response = llama3(prompt)\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e463097-c832-4a81-9e73-91da413ed978",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_xml_string(xml_string):\n",
    "    \"\"\"\n",
    "    Clean the XML string to ensure it is well-formed.\n",
    "    \"\"\"\n",
    "    # Remove leading/trailing whitespace\n",
    "    xml_string = xml_string.strip()\n",
    "    \n",
    "    # Normalize the XML string\n",
    "    xml_string = xml_string.replace('-', '_').lower()\n",
    "    xml_string = xml_string.replace('pulmonary edema', 'pulmonary_edema').lower()\n",
    "    xml_string = xml_string.replace('pulmonaryedema', 'pulmonary_edema').lower()\n",
    "    xml_string = xml_string.replace('pleural effusion', 'pleural_effusion').lower()\n",
    "    xml_string = xml_string.replace('pleuraleffusion', 'pleural_effusion').lower()\n",
    "    \n",
    "    # Additional cleaning steps can be added here if needed\n",
    "    \n",
    "    return xml_string\n",
    "\n",
    "def extract_abnormalities_from_xml(xml_string):\n",
    "    \"\"\"\n",
    "    This function extracts abnormalities and their values from the given XML string.\n",
    "    \"\"\"\n",
    "    # Clean the XML string\n",
    "    xml_string = clean_xml_string(xml_string)\n",
    "    \n",
    "    # Define the list of abnormalities we're interested in\n",
    "    abnormalities = [\"pulmonary_edema\", \"consolidation\", \"pleural_effusion\", \"pneumothorax\", \"cardiomegaly\"]\n",
    "    \n",
    "    # Initialize the results dictionary\n",
    "    results = {abnormality: 0 for abnormality in abnormalities}\n",
    "    \n",
    "    try:\n",
    "        # Parse the XML\n",
    "        root = ET.fromstring(xml_string)\n",
    "        \n",
    "        # Extract values\n",
    "        for abnormality in abnormalities:\n",
    "            element = root.find(f\".//{abnormality}\")\n",
    "            if element is not None:\n",
    "                results[abnormality] = int(element.text.strip())\n",
    "    except ET.ParseError as e:\n",
    "        print(f\"Error parsing XML: {e}\")\n",
    "        print(f\"XML string: {xml_string}\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb9e282-49fc-4bfb-953a-a525787712a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_radiology_reports(df):\n",
    "    \"\"\"\n",
    "    This function processes the radiology reports in the dataframe and extracts the abnormalities.\n",
    "    \"\"\"\n",
    "    # Initialize a list to store the results\n",
    "    data = []\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        try:\n",
    "            report_text = row['Text']\n",
    "\n",
    "            # Here we assume `run_llama3` is your function that processes the report text and returns the XML\n",
    "            xml_output = classify_abnormalities(report_text)\n",
    "\n",
    "            # Extract abnormalities from the XML\n",
    "            abnormalities = extract_abnormalities_from_xml(xml_output)\n",
    "\n",
    "            # Combine the original text with the extracted abnormalities\n",
    "            data.append({**{'Text': report_text}, **abnormalities})\n",
    "        except:\n",
    "            print(f'WARNING! Issue with index: {index}')\n",
    "    \n",
    "    # Create a new dataframe from the results\n",
    "    new_df = pd.DataFrame(data)\n",
    "    \n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dea374c-5b54-414a-8be5-6f9246f0dca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample = df[:50].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6033d5ce-b728-4bd7-b79e-8e6461075b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rad = process_radiology_reports(df_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ed7dba-409d-48a2-a666-98222d2e4361",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b06415-cca2-42ac-bf40-85afe15cfaac",
   "metadata": {},
   "source": [
    "### Save the labels to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32253b4b-a515-45ea-9e6b-5ed151542a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rad.to_csv('../data/report_pseudo_labels_llama3.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f18fcb4-8f04-4b5e-be12-d011981797a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample['Text'][12]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b9b403-df5a-4283-937f-442af0be4f3d",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353d5d28-f28b-4cda-944d-8efe967f19df",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rad = pd.read_csv('../data/report_pseudo_labels_llama3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a81dbb-48ba-4097-8428-c551ed44c80e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf11a83c-e1d3-4d88-aa2b-c4bc64d4812a",
   "metadata": {},
   "source": [
    "# Train a model on our pseudo labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eaef8de-2eea-4de5-955e-b3497d810076",
   "metadata": {},
   "source": [
    "## Option 1: Fine Tune"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da514fba-a70d-4f57-8205-46361b92b781",
   "metadata": {},
   "source": [
    "### Step 1: Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f01d8d-c23f-4dde-9113-e2eb3421e901",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "# Load your dataframe\n",
    "df = pd.read_csv('path_to_your_dataframe.csv')\n",
    "\n",
    "# Ensure the columns are in the correct format\n",
    "df['Text'] = df['Text'].astype(str)\n",
    "df['pulmonary_edema'] = df['pulmonary_edema'].astype(int)\n",
    "df['consolidation'] = df['consolidation'].astype(int)\n",
    "df['pleural_effusion'] = df['pleural_effusion'].astype(int)\n",
    "df['pneumothorax'] = df['pneumothorax'].astype(int)\n",
    "df['cardiomegaly'] = df['cardiomegaly'].astype(int)\n",
    "\n",
    "# Split the dataframe into training and validation sets\n",
    "train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert pandas dataframe to Hugging Face Dataset\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "val_dataset = Dataset.from_pandas(val_df)\n",
    "\n",
    "# Create a DatasetDict\n",
    "dataset_dict = DatasetDict({\n",
    "    'train': train_dataset,\n",
    "    'validation': val_dataset\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0a0440-b5a7-41fc-b94d-7f778fc38eb4",
   "metadata": {},
   "source": [
    "### Step 2: Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4957902-4913-4a78-a741-328c6e211415",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"RLHFlow/ArmoRM-Llama3-8B-v0.1\", use_fast=True)\n",
    "\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example['Text'], padding=\"max_length\", truncation=True, max_length=512)\n",
    "\n",
    "# Apply tokenization to the dataset\n",
    "tokenized_datasets = dataset_dict.map(tokenize_function, batched=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce5767e-9a40-4fec-9721-ba6d8b2a18a2",
   "metadata": {},
   "source": [
    "### Step 3: Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93330b68-1c54-43ee-adef-e6a9d06dbada",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"RLHFlow/ArmoRM-Llama3-8B-v0.1\",\n",
    "    num_labels=5,  # Number of labels (one for each abnormality)\n",
    "    problem_type=\"multi_label_classification\"\n",
    ")\n",
    "\n",
    "# Define the columns to keep and set the format for PyTorch\n",
    "tokenized_datasets = tokenized_datasets.remove_columns([\"Text\"])\n",
    "tokenized_datasets.set_format(\"torch\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a731e2-60fc-4365-8c51-c52aa6ff8ad9",
   "metadata": {},
   "source": [
    "### Step 4: Define Training Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76294ac9-da04-4c70-a147-2248769ca33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e187cf-e273-439f-be87-ef9a95f68593",
   "metadata": {},
   "source": [
    "### Step 5: Define the Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b402bc-f739-45ed-93cd-08de1c67ca2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='micro')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45cc55a4-591e-4843-9590-552e6027a774",
   "metadata": {},
   "source": [
    "### Step 6: Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11b6ef0-8971-4f83-ac87-43762e4f0669",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ba0096-43b7-4f6a-b31c-77331253c6ff",
   "metadata": {},
   "source": [
    "## Option 2: Pre-trained model feature extractor\n",
    "\n",
    "This portion will largely follow chapter 2 of Natural Language Processing with Transformers by Tunstall, Werra, and Wolf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda5f6b5-def2-4bda-a466-44b75f6e0163",
   "metadata": {},
   "source": [
    "To get code working and as a guide we can use the emotions dataset, which looks at classifying the emotion assocated with Twitter messages and is available from Hugging Face Hub. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89d7431-5424-4519-b05b-79b9565fc6ea",
   "metadata": {},
   "source": [
    "### Look at class distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf7da75-52ee-4993-be25-aa2f6dae18b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239df15d-9b76-4af9-a1f9-6b4d8fe055dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "abnormalities = ['consolidation', 'pneumothorax', 'cardiomegaly']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d4beae-f912-4745-8101-15e85e986551",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the occurrences of 1s and 0s for each column\n",
    "counts = df_rad[abnormalities].apply(pd.Series.value_counts).T\n",
    "counts.columns = ['0', '1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a07cf3e-808e-47fc-905b-de10f71560e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8251662d-161a-435d-9af5-8c93f8e04cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f566cc-97be-403b-9a8a-a5b8d4738a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the counts\n",
    "counts.plot(kind='bar', stacked=True)\n",
    "plt.title('Distribution of Conditions')\n",
    "plt.xlabel('Condition')\n",
    "plt.ylabel('Count')\n",
    "plt.legend(title='Value')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e50d3d-6299-472d-9233-d5714803a1e5",
   "metadata": {},
   "source": [
    "### Look at length of radiology reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640ccf63-d9b3-402c-94bc-9a316381202c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rad['words per report'] = df_rad['Text'].str.split().apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ca608d-261f-486b-be81-188e6ae543f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the histogram for the \"words per report\" column\n",
    "df_rad['words per report'].hist(bins=10, edgecolor='black')\n",
    "\n",
    "# Add titles and labels\n",
    "plt.title('Distribution of Words per Report')\n",
    "plt.xlabel('Words per Report')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7fd10f-b33a-4e10-a02e-cbca3ada3482",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dceac2c0-b16a-4f08-b3a5-babe960efa16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "from torch import Tensor\n",
    "from transformers import AutoTokenizer, AutoModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c789b963-3c8f-4a7c-9f55-90aa78595b23",
   "metadata": {},
   "source": [
    "### Model setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea42181-e69d-405e-a175-a12c2fcc13a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = 'intfloat/e5-small-v2'\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc4816f-b15d-4d95-9a11-619a86dfc53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModel.from_pretrained(model_id).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fad6ce5-fff6-4ee9-a4bf-2196149f86e3",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13871dcc-c918-4afb-a2dd-2599dc382162",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rad['Text'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e677be14-57ee-443f-a5d1-1512bf4643ae",
   "metadata": {},
   "source": [
    "Get the encoded token ids for the text above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3c669e-34e7-430a-9d60-c4cddc1dafe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_text = tokenizer(df_rad['Text'][0])\n",
    "print(encoded_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9a03fa-e703-4688-9ea4-0ff4162a779d",
   "metadata": {},
   "source": [
    "Let's see how the original text was tokenized into words and subwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22cc8736-437e-49b2-b47b-39039dccbb6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer.convert_ids_to_tokens(tokenizer(df_rad['Text'][0]).input_ids)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0185ae14-033c-429a-96e1-1e69571494e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.convert_tokens_to_string(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81f0aca-f222-460d-89f4-6c3adc8d1ae8",
   "metadata": {},
   "source": [
    "The tokenizer also has a few useful attributes to understand its properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a9304b-b05f-4bcd-b038-78e27c8619d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6545e8-a248-4079-b96e-e24d18b2abaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.model_max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f50e54-ab69-4502-bc0c-e1884cf56aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.model_input_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2578dac-02b0-42b3-a8e9-61cf6bd2be4a",
   "metadata": {},
   "source": [
    "### Get stats around tokenization length of the reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc4396c-56e7-438b-ba4a-c191e4853449",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_token_length(text):\n",
    "    tokens = tokenizer(text)\n",
    "    return len(tokens['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e70da0-f91d-4a6d-b27a-6247abab5598",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rad['token_length'] = df_rad['Text'].apply(get_token_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6cb32a-1890-47ea-8a3b-8e67b063a2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rad['token_length'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a9cee8-76af-486a-82f6-1361a60c7dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the histogram for the \"tokens per report\" column\n",
    "df_rad['token_length'].hist(bins=10, edgecolor='black')\n",
    "\n",
    "# Add titles and labels\n",
    "plt.title('Distribution of Tokens per Report')\n",
    "plt.xlabel('Tokens per Report')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c71807-f39b-48b3-b335-f5381d41b776",
   "metadata": {},
   "source": [
    "### Extracting the last hidden state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115688ee-85ea-4be1-bb3b-a2cddae2eabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"some sample text\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f594c052-208b-446a-bc62-889659dc18af",
   "metadata": {},
   "source": [
    "Note that the hidden state or embedding vector on the class token is being used here. This class token is the one typically used for classification tasks. We will start by using that here as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca67ccd-f399-4b2e-97da-4e49c9968531",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_hidden_states(batch):\n",
    "    inputs = {k:v.to(device) for k,v in batch.items() if k in tokenizer.model_input_names}\n",
    "    with torch.no_grad():\n",
    "        last_hidden_state = model(**inputs).last_hidden_state\n",
    "    return {\"hidden_state\": last_hidden_state[:,0].cpu().numpy()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52c6a61-0ab6-4106-9e91-8050739c11cf",
   "metadata": {},
   "source": [
    "### Create feature matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf4b442-6a2e-4316-b9d3-94c430eba307",
   "metadata": {},
   "source": [
    "### Train model on the extracted features\n",
    "\n",
    "We could use a simple fully connected model where the final output has `sigmoid` activation function. Or we could use an ensemble model (e.g. xgboost)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47511821-5263-4d19-9d0c-d98935b95a92",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
