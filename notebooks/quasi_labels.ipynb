{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b987e7e5-483f-48d5-9e4c-60963dc85219",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import re\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from typing import List\n",
    "from utils import json_to_dataframe, json_to_string_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73ade26-8fcf-4056-8874-4e6f2a6dfce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a central location for storing models\n",
    "CENTRAL_MODEL_DIR = os.path.expanduser('~/huggingface_models')\n",
    "\n",
    "# model_name = 'microsoft/phi-2'\n",
    "# model_name = 'microsoft/phi-1_5'\n",
    "# model_name = 'microsoft/Phi-3.5-mini-instruct'\n",
    "# model_name = 'google/gemma-2-9b'\n",
    "# model_name = 'meta-llama/Meta-Llama-3.1-8B'\n",
    "# model_name = 'meta-llama/Meta-Llama-3.1-8B-Instruct' # Downloaded locally (slow)\n",
    "model_name = 'google/gemma-2-2b-it' # Downloaded locally\n",
    "# model_name = 'google/gemma-2-9b-it' # Downloaded locally (slow)\n",
    "# model_name = 'Qwen/Qwen2.5-7B-Instruct' # Downloaded locally (slow but faster than gemma 2 9b and llama 3.1 8b)\n",
    "# model_name = 'meta-llama/Llama-3.2-3B-Instruct'\n",
    "# model_name = 'meta-llama/Llama-3.2-1B-Instruct'\n",
    "\n",
    "# Create the central directory if it doesn't exist\n",
    "os.makedirs(CENTRAL_MODEL_DIR, exist_ok=True)\n",
    "\n",
    "# Define the path where the model will be saved locally\n",
    "local_model_path = os.path.join(CENTRAL_MODEL_DIR, model_name.replace('/', '-'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d46126-e530-467c-b814-2dc7b636e6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automatically detect and use GPU if available, otherwise use CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Set up the device map\n",
    "if torch.cuda.is_available():\n",
    "    device_map = \"auto\"  # This will automatically distribute the model across available GPUs\n",
    "else:\n",
    "    device_map = {\"\": device}  # Use the detected device (CPU in this case)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c05491a3-fbed-4a61-b80d-2c9fcec05373",
   "metadata": {},
   "source": [
    "## Optional load quantized model\n",
    "\n",
    "This may be a good solution in cases where the full model won't fit into GPU memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c913b49-1c2a-4a62-a494-5a43ae1e44fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "# from bitsandbytes.nn import Linear8bitLt\n",
    "\n",
    "# def load_quantized_model(model_name, quantization_bit=8):\n",
    "#     # Ensure bitsandbytes is installed\n",
    "#     try:\n",
    "#         import bitsandbytes as bnb\n",
    "#     except ImportError:\n",
    "#         raise ImportError(\"Please install bitsandbytes: pip install bitsandbytes\")\n",
    "\n",
    "#     # Set up quantization configuration\n",
    "#     if quantization_bit == 8:\n",
    "#         bnb_config = {'load_in_8bit': True}\n",
    "#     elif quantization_bit == 4:\n",
    "#         bnb_config = {'load_in_4bit': True}\n",
    "#     else:\n",
    "#         raise ValueError(\"Quantization bit must be 4 or 8\")\n",
    "\n",
    "#     # Load the tokenizer\n",
    "#     tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "#     # Load the quantized model\n",
    "#     model = AutoModelForCausalLM.from_pretrained(\n",
    "#         model_name,\n",
    "#         device_map=\"auto\",\n",
    "#         quantization_config=bnb_config,\n",
    "#         trust_remote_code=True\n",
    "#     )\n",
    "\n",
    "#     return model, tokenizer\n",
    "\n",
    "# # Usage\n",
    "# model_name = 'Qwen/Qwen2.5-7B-Instruct'\n",
    "# quantized_model, tokenizer = load_quantized_model(model_name, quantization_bit=8)\n",
    "\n",
    "# # You can now use quantized_model instead of original_model in your code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df24913-5b72-4bd3-ad42-3774e7ed53f1",
   "metadata": {},
   "source": [
    "### Optional huggingface login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1d40b8-2edd-4207-a306-5d78b75fb709",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from huggingface_hub import notebook_login\n",
    "# notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6449b3e5-2c29-45dc-8f43-761a96d575ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the model exists locally\n",
    "if os.path.exists(local_model_path):\n",
    "    print(f\"Loading model from local path: {local_model_path}\")\n",
    "    original_model = AutoModelForCausalLM.from_pretrained(\n",
    "        local_model_path,\n",
    "        device_map=device_map,\n",
    "        # quantization_config=bnb_config,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "else:\n",
    "    print(f\"Downloading model from {model_name}\")\n",
    "    original_model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        device_map=device_map,\n",
    "        # quantization_config=bnb_config,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    # Save the model locally\n",
    "    original_model.save_pretrained(local_model_path)\n",
    "    print(f\"Model saved to {local_model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b53ecb2-99a8-47be-9f88-fd434cdc92a3",
   "metadata": {},
   "source": [
    "NOTE: If there is a warning above about offloading onto CPU, then the model will run verrrry slooowwwly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe0c96a-d213-4cdb-91d5-7db604af098a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_llm_function(model, tokenizer, max_new_tokens=512, temperature=0.7):\n",
    "    def llm_function(prompt: str) -> str:\n",
    "        # Tokenize the input prompt\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "        # Generate the output\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                temperature=temperature,\n",
    "                do_sample=True,\n",
    "                pad_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "\n",
    "        # Decode the output\n",
    "        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "        # Extract the model's response (everything after the prompt)\n",
    "        response = generated_text[len(prompt):].strip()\n",
    "\n",
    "        return response\n",
    "\n",
    "    return llm_function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2989a784-6389-46a8-a7b5-e22fb839d1c9",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41172be6-750e-470d-a0d3-9a5b8b279d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load public radiology reports dataset\n",
    "# df = pd.read_csv('../data/ReportsDATASET.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79592109-7fa7-4f3e-b2fa-671d26e4fad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = '../data/vector_veterinary_imaging_2.json'\n",
    "\n",
    "df = json_to_dataframe(filepath) \n",
    "rad_strings = json_to_string_list(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5f79ec-040b-464e-bc80-8e25fcc86ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe4f1fe-7b67-47dd-bb9c-0d5569259f5e",
   "metadata": {},
   "source": [
    "## Define pre and post processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cca0e91-55f0-4aad-a8f8-35f73af3b108",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_radiology_report(report: str) -> str:\n",
    "    \"\"\"\n",
    "    Preprocesses a radiology report by removing unnecessary whitespace,\n",
    "    newline characters, and potential HTML tags.\n",
    "\n",
    "    Args:\n",
    "    report (str): The original radiology report text\n",
    "\n",
    "    Returns:\n",
    "    str: The preprocessed radiology report text\n",
    "    \"\"\"\n",
    "    # Remove any HTML tags\n",
    "    report = re.sub(r'<[^>]+>', '', report)\n",
    "    \n",
    "    # Replace multiple newlines and spaces with a single space\n",
    "    report = re.sub(r'\\s+', ' ', report)\n",
    "    \n",
    "    # Remove leading and trailing whitespace\n",
    "    report = report.strip()\n",
    "    \n",
    "    # Replace 'XXXX' with a placeholder like '[REDACTED]'\n",
    "    report = re.sub(r'XXXX', '[REDACTED]', report)\n",
    "    \n",
    "    return report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3b4f9d-4b86-438e-8538-687950baad36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example_report = df['Text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad21bcd4-b941-416a-b8a6-4fdc5a884f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessed_report = preprocess_radiology_report(example_report)\n",
    "# print(preprocessed_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ee6508-9bbf-4b55-ae41-32464aa0c1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_process_abnormalities(classification_result: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Takes the JSON string output from classify_abnormalities and returns a list of\n",
    "    abnormalities that are present (have a value of 1).\n",
    "\n",
    "    Args:\n",
    "    classification_result (str): JSON string output from classify_abnormalities\n",
    "\n",
    "    Returns:\n",
    "    List[str]: List of abnormalities that are present\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Parse the JSON string into a dictionary\n",
    "        result_dict = json.loads(classification_result)\n",
    "        \n",
    "        # Filter the dictionary for keys with value 1\n",
    "        present_abnormalities = [abnormality for abnormality, value in result_dict.items() if value == 1]\n",
    "        \n",
    "        return present_abnormalities\n",
    "    except json.JSONDecodeError:\n",
    "        raise ValueError(\"Invalid JSON string provided\")\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Error processing classification result: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5038f359-0429-4c2c-9b24-6d92b6d3276f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_process_llm_output(output: str) -> str:\n",
    "    \"\"\"\n",
    "    Clean up the LLM output by removing code block markers and newlines.\n",
    "    \"\"\"\n",
    "    # Remove code block markers\n",
    "    output = re.sub(r'```(?:json)?\\s*', '', output)\n",
    "    output = output.replace('`', '')\n",
    "    \n",
    "    # Remove newlines\n",
    "    output = output.replace('\\n', ' ')\n",
    "    \n",
    "    # Remove any leading/trailing whitespace\n",
    "    output = output.strip()\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c0a5383-63b8-47a1-bcb0-68650be2bc61",
   "metadata": {},
   "source": [
    "## Generate prompt to identify abnormalities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980efb98-5103-438b-8018-ce66a676c5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_abnormalities(abnormalities: List[str], report: str, llm_function) -> str:\n",
    "    # Preprocess the report\n",
    "    preprocessed_report = preprocess_radiology_report(report)\n",
    "\n",
    "    # Create a dynamic prompt for the LLM\n",
    "    prompt = f\"\"\"\n",
    "Given the following radiology report, classify the presence (1) or absence (0) of the specified abnormalities.\n",
    "Output the result as a JSON string without any additional explanation.\n",
    "\n",
    "Abnormalities to classify: {', '.join(abnormalities)}\n",
    "\n",
    "Radiology report:\n",
    "{preprocessed_report}\n",
    "\n",
    "Output format:\n",
    "{{\n",
    "    \"abnormality1\": 0 or 1,\n",
    "    \"abnormality2\": 0 or 1,\n",
    "    ...\n",
    "}}\n",
    "Return a JSON string without any explanation.\n",
    "\"\"\"\n",
    "\n",
    "    # Call the LLM function with the prompt\n",
    "    llm_output = llm_function(prompt)\n",
    "\n",
    "    # Post-process the LLM output\n",
    "    llm_output = post_process_llm_output(llm_output)\n",
    "\n",
    "    # Ensure the output is valid JSON\n",
    "    try:\n",
    "        result = json.loads(llm_output)\n",
    "        # Verify that all abnormalities are present in the output\n",
    "        for abnormality in abnormalities:\n",
    "            if abnormality not in result:\n",
    "                print(result)\n",
    "                raise ValueError(f\"Missing abnormality in LLM output: {abnormality}\")\n",
    "        return json.dumps(result)\n",
    "    except json.JSONDecodeError:\n",
    "        print(result)\n",
    "        raise ValueError(\"LLM output is not valid JSON\")\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Error processing LLM output: {str(e)}\")\n",
    "\n",
    "# Example usage \n",
    "def mock_llm_function(prompt: str) -> str:\n",
    "    # This is a mock function that simulates an LLM's response\n",
    "    return '{\"pulmonary edema\": 1, \"consolidation\": 0, \"pleural effusion\": 1, \"pneumothorax\": 0, \"cardiomegaly\": 1}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0657c4-8351-495d-bc02-2466026211da",
   "metadata": {},
   "outputs": [],
   "source": [
    "abnormalities = [\"pulmonary edema\", \"consolidation\", \"pleural effusion\", \"pneumothorax\", \"cardiomegaly\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0e8639-159d-46ae-8007-6cada3a12fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "# result = classify_abnormalities(abnormalities, example_report, mock_llm_function)\n",
    "# print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e952603-03cd-41ac-b13f-d0b72e05ac6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the post-processing function\n",
    "# present_abnormalities = post_process_abnormalities(result)\n",
    "# print(\"Present abnormalities:\", present_abnormalities)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85be8467-6d44-40c9-9929-6bbea08bf862",
   "metadata": {},
   "source": [
    "## Create the LLM inference function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "600cdc29-07ea-44c9-9b88-dc1e12134af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Create the LLM function\n",
    "llm_function = create_llm_function(original_model, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d6e9de-e0cf-4b70-aac0-54289ba11179",
   "metadata": {},
   "source": [
    "## Run a single example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206902e1-184d-4d35-83d0-e9ee6d2e6216",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example_2 = df['Text'][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ab272a-7531-416a-bbcc-da173bba8b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72fdb6ab-7ede-4d1c-8118-6a3f604cf098",
   "metadata": {},
   "outputs": [],
   "source": [
    "# result = classify_abnormalities(abnormalities, example_report, llm_function)\n",
    "# print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e511249-e385-4c43-b391-958231638566",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(example_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c02f235-0a30-41bf-a1de-ccad242e8ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# result = classify_abnormalities(abnormalities, example_2, llm_function)\n",
    "# print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f01ff55-a223-458e-b73c-83896718b350",
   "metadata": {},
   "source": [
    "## Run the model on a full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4e0889-217b-44ad-8c2f-a469711a623e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_radiology_reports(df, abnormalities, llm_function):\n",
    "    # Create new columns for each abnormality, initialized with 0\n",
    "    for abnormality in abnormalities:\n",
    "        df[abnormality] = 0\n",
    "    \n",
    "    # Create a tqdm progress bar\n",
    "    for index, row in tqdm(df.iterrows(), total=df.shape[0], desc=\"Processing reports\"):\n",
    "        report = row['conclusions_and_recommendations']\n",
    "        \n",
    "        try:\n",
    "            # Classify abnormalities\n",
    "            classification_result = classify_abnormalities(abnormalities, report, llm_function)\n",
    "            \n",
    "            # Post-process the classification result\n",
    "            present_abnormalities = post_process_abnormalities(classification_result)\n",
    "            \n",
    "            # Update the DataFrame\n",
    "            for abnormality in present_abnormalities:\n",
    "                df.at[index, abnormality] = 1\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing report at index {index}: {str(e)}\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a9a2ab-472a-424b-afd8-8b85fbb9f48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194ea95e-0eab-44c2-bf11-ec383b3518ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = process_radiology_reports(df, abnormalities, llm_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bebbe033-ab42-4afa-9fad-455823fb1a95",
   "metadata": {},
   "source": [
    "## Optional Run batched inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aafe63fe-314d-46c7-87dc-0a90662c7461",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_batch_llm_function(model, tokenizer, max_new_tokens=512, temperature=0.7, batch_size=8):\n",
    "#    def batch_llm_function(prompts: List[str]) -> List[str]:\n",
    "#        # Tokenize all prompts\n",
    "#        inputs = tokenizer(prompts, return_tensors=\"pt\", padding=True, truncation=True).to(model.device)\n",
    "       \n",
    "#        # Generate outputs for the entire batch\n",
    "#        with torch.no_grad():\n",
    "#            outputs = model.generate(\n",
    "#                **inputs,\n",
    "#                max_new_tokens=max_new_tokens,\n",
    "#                temperature=temperature,\n",
    "#                do_sample=True,\n",
    "#                pad_token_id=tokenizer.eos_token_id\n",
    "#            )\n",
    "       \n",
    "#        # Decode all outputs\n",
    "#        generated_texts = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "       \n",
    "#        # Extract responses (everything after the respective prompts)\n",
    "#        responses = [text[len(prompt):].strip() for text, prompt in zip(generated_texts, prompts)]\n",
    "       \n",
    "#        return responses\n",
    "   \n",
    "#    def process_in_batches(all_prompts: List[str]) -> List[str]:\n",
    "#        all_responses = []\n",
    "#        for i in range(0, len(all_prompts), batch_size):\n",
    "#            batch = all_prompts[i:i+batch_size]\n",
    "#            responses = batch_llm_function(batch)\n",
    "#            all_responses.extend(responses)\n",
    "#        return all_responses\n",
    "   \n",
    "#    return process_in_batches\n",
    "\n",
    "# def process_radiology_reports_batch(df, abnormalities, batch_llm_function):\n",
    "#    for abnormality in abnormalities:\n",
    "#        df[abnormality] = 0\n",
    "   \n",
    "#    all_reports = df['conclusions_and_recommendations'].tolist()\n",
    "#    all_prompts = [f\"Given the following radiology report, classify the presence (1) or absence (0) of the specified abnormalities. Output the result as a JSON string without any additional explanation.\\n\\nAbnormalities to classify: {', '.join(abnormalities)}\\n\\nRadiology report:\\n{preprocess_radiology_report(report)}\\n\\nOutput format:\\n{{\\n    \\\"abnormality1\\\": 0 or 1,\\n    \\\"abnormality2\\\": 0 or 1,\\n    ...\\n}}\\nReturn a JSON string without any explanation.\" for report in all_reports]\n",
    "   \n",
    "#    all_results = batch_llm_function(all_prompts)\n",
    "   \n",
    "#    for index, result in enumerate(all_results):\n",
    "#        try:\n",
    "#            classification_result = post_process_llm_output(result)\n",
    "#            present_abnormalities = post_process_abnormalities(classification_result)\n",
    "#            for abnormality in present_abnormalities:\n",
    "#                df.at[index, abnormality] = 1\n",
    "#        except Exception as e:\n",
    "#            print(f\"Error processing report at index {index}: {str(e)}\")\n",
    "   \n",
    "#    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c437ad-24e3-44fc-881f-f54b878b3970",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_llm_function = create_batch_llm_function(original_model, tokenizer)\n",
    "\n",
    "# df = process_radiology_reports_batch(df, abnormalities, batch_llm_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0783edbf-0c5f-4c39-b373-98b4e5ac474d",
   "metadata": {},
   "source": [
    "## Save the results locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c966fed7-ec93-4e43-95f9-333cd6e2be23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def save_labeled_dataframe(df, model_name, base_path='./labeled_data'):\n",
    "    # Clean up the model name for use in filename\n",
    "    clean_model_name = model_name.replace('/', '_').replace('\\\\', '_')\n",
    "    \n",
    "    # Get current date\n",
    "    current_date = datetime.now().strftime('%Y%m%d')\n",
    "    \n",
    "    # Create filename\n",
    "    filename = f\"{clean_model_name}_model_labeled_{current_date}.csv\"\n",
    "    \n",
    "    # Ensure the base path exists\n",
    "    os.makedirs(base_path, exist_ok=True)\n",
    "    \n",
    "    # Full path for the file\n",
    "    full_path = os.path.join(base_path, filename)\n",
    "    \n",
    "    # Save DataFrame to CSV without index\n",
    "    df.to_csv(full_path, index=False)\n",
    "    \n",
    "    print(f\"DataFrame saved to {full_path}\")\n",
    "    \n",
    "    return full_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ac3f05-452d-4d32-8358-df2555c9a8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_path = save_labeled_dataframe(df, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316ba215-0997-413f-b89c-0e918c9c9429",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
