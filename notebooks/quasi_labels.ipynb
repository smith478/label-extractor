{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f292b0e8-45d5-4858-932c-cbd433351910",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db01627a-bfac-4ba2-84ee-a30481336d3c",
   "metadata": {},
   "source": [
    "## Download or load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29cacbc1-9128-48d6-9c6e-9ef3fef83f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a central location for storing models\n",
    "CENTRAL_MODEL_DIR = os.path.expanduser('~/huggingface_models')\n",
    "\n",
    "# model_name = 'microsoft/phi-2'\n",
    "# model_name = 'microsoft/phi-1_5'\n",
    "# model_name = 'microsoft/Phi-3.5-mini-instruct'\n",
    "# model_name = 'google/gemma-2-9b'\n",
    "# model_name = 'meta-llama/Meta-Llama-3.1-8B'\n",
    "# model_name = 'meta-llama/Meta-Llama-3.1-8B-Instruct'\n",
    "model_name = 'google/gemma-2-2b-it'\n",
    "# model_name = 'google/gemma-2-9b-it'\n",
    "\n",
    "# Create the central directory if it doesn't exist\n",
    "os.makedirs(CENTRAL_MODEL_DIR, exist_ok=True)\n",
    "\n",
    "# Define the path where the model will be saved locally\n",
    "local_model_path = os.path.join(CENTRAL_MODEL_DIR, model_name.replace('/', '-'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0be91bb-7c7e-480d-b941-77f79d1ebc41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automatically detect and use GPU if available, otherwise use CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Set up the device map\n",
    "if torch.cuda.is_available():\n",
    "    device_map = \"auto\"  # This will automatically distribute the model across available GPUs\n",
    "else:\n",
    "    device_map = {\"\": device}  # Use the detected device (CPU in this case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c5e9d1e-6df6-4322-9522-c68a366a0c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the model exists locally\n",
    "if os.path.exists(local_model_path):\n",
    "    print(f\"Loading model from local path: {local_model_path}\")\n",
    "    original_model = AutoModelForCausalLM.from_pretrained(\n",
    "        local_model_path,\n",
    "        device_map=device_map,\n",
    "        # quantization_config=bnb_config,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "else:\n",
    "    print(f\"Downloading model from {model_name}\")\n",
    "    original_model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        device_map=device_map,\n",
    "        # quantization_config=bnb_config,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    # Save the model locally\n",
    "    original_model.save_pretrained(local_model_path)\n",
    "    print(f\"Model saved to {local_model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82791140-6d49-485b-94dd-b8e64bf2da3f",
   "metadata": {},
   "source": [
    "## Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ce9b7e-f485-4448-b2e4-118038cf1e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/ReportsDATASET.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2dca69-8281-43c3-aeed-831d8dcc8139",
   "metadata": {},
   "source": [
    "## Define the labeling prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "376d28c3-b325-4fd4-abfe-d6669fb7ea40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_abnormalities(abnormalities: List[str], report: str, llm_function) -> str:\n",
    "    # Create a dynamic prompt for the LLM\n",
    "    prompt = f\"\"\"\n",
    "Given the following radiology report, classify the presence (1) or absence (0) of the specified abnormalities.\n",
    "Output the result as a JSON string without any additional explanation.\n",
    "\n",
    "Abnormalities to classify: {', '.join(abnormalities)}\n",
    "\n",
    "Radiology report:\n",
    "{report}\n",
    "\n",
    "Output format:\n",
    "{{\n",
    "    \"abnormality1\": 0 or 1,\n",
    "    \"abnormality2\": 0 or 1,\n",
    "    ...\n",
    "}}\n",
    "\n",
    "The output must be a JSON string without any explanation.\n",
    "\"\"\"\n",
    "\n",
    "    # Call the LLM function with the prompt\n",
    "    llm_output = llm_function(prompt)\n",
    "\n",
    "    # Ensure the output is valid JSON\n",
    "    try:\n",
    "        result = json.loads(llm_output)\n",
    "        # Verify that all abnormalities are present in the output\n",
    "        for abnormality in abnormalities:\n",
    "            if abnormality not in result:\n",
    "                raise ValueError(f\"Missing abnormality in LLM output: {abnormality}\")\n",
    "        return json.dumps(result)\n",
    "    except json.JSONDecodeError:\n",
    "        raise ValueError(\"LLM output is not valid JSON\")\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Error processing LLM output: {str(e)}\")\n",
    "\n",
    "# Example usage \n",
    "def mock_llm_function(prompt: str) -> str:\n",
    "    # This is a mock function that simulates an LLM's response\n",
    "    return '{\"pulmonary edema\": 1, \"consolidation\": 0, \"pleural effusion\": 1, \"pneumothorax\": 0, \"cardiomegaly\": 1}'\n",
    "\n",
    "# Example usage\n",
    "abnormalities = [\"pulmonary edema\", \"consolidation\", \"pleural effusion\", \"pneumothorax\", \"cardiomegaly\"]\n",
    "report = \"The chest radiograph shows increased opacification in the lower lung fields bilaterally, consistent with pulmonary edema. There is also evidence of pleural effusion and an enlarged cardiac silhouette suggestive of cardiomegaly.\"\n",
    "\n",
    "result = classify_abnormalities(abnormalities, report, mock_llm_function)\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
