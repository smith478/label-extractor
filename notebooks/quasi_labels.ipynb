{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b987e7e5-483f-48d5-9e4c-60963dc85219",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import re\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73ade26-8fcf-4056-8874-4e6f2a6dfce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a central location for storing models\n",
    "CENTRAL_MODEL_DIR = os.path.expanduser('~/huggingface_models')\n",
    "\n",
    "# model_name = 'microsoft/phi-2'\n",
    "# model_name = 'microsoft/phi-1_5'\n",
    "# model_name = 'microsoft/Phi-3.5-mini-instruct'\n",
    "# model_name = 'google/gemma-2-9b'\n",
    "# model_name = 'meta-llama/Meta-Llama-3.1-8B'\n",
    "# model_name = 'meta-llama/Meta-Llama-3.1-8B-Instruct'\n",
    "# model_name = 'google/gemma-2-2b-it'\n",
    "model_name = 'google/gemma-2-9b-it'\n",
    "\n",
    "# Create the central directory if it doesn't exist\n",
    "os.makedirs(CENTRAL_MODEL_DIR, exist_ok=True)\n",
    "\n",
    "# Define the path where the model will be saved locally\n",
    "local_model_path = os.path.join(CENTRAL_MODEL_DIR, model_name.replace('/', '-'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d46126-e530-467c-b814-2dc7b636e6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automatically detect and use GPU if available, otherwise use CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Set up the device map\n",
    "if torch.cuda.is_available():\n",
    "    device_map = \"auto\"  # This will automatically distribute the model across available GPUs\n",
    "else:\n",
    "    device_map = {\"\": device}  # Use the detected device (CPU in this case)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df24913-5b72-4bd3-ad42-3774e7ed53f1",
   "metadata": {},
   "source": [
    "### Optional huggingface login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1d40b8-2edd-4207-a306-5d78b75fb709",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from huggingface_hub import notebook_login\n",
    "# notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6449b3e5-2c29-45dc-8f43-761a96d575ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the model exists locally\n",
    "if os.path.exists(local_model_path):\n",
    "    print(f\"Loading model from local path: {local_model_path}\")\n",
    "    original_model = AutoModelForCausalLM.from_pretrained(\n",
    "        local_model_path,\n",
    "        device_map=device_map,\n",
    "        # quantization_config=bnb_config,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "else:\n",
    "    print(f\"Downloading model from {model_name}\")\n",
    "    original_model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        device_map=device_map,\n",
    "        # quantization_config=bnb_config,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    # Save the model locally\n",
    "    original_model.save_pretrained(local_model_path)\n",
    "    print(f\"Model saved to {local_model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe0c96a-d213-4cdb-91d5-7db604af098a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_llm_function(model, tokenizer, max_new_tokens=512, temperature=0.7):\n",
    "    def llm_function(prompt: str) -> str:\n",
    "        # Tokenize the input prompt\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "        # Generate the output\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                temperature=temperature,\n",
    "                do_sample=True,\n",
    "                pad_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "\n",
    "        # Decode the output\n",
    "        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "        # Extract the model's response (everything after the prompt)\n",
    "        response = generated_text[len(prompt):].strip()\n",
    "\n",
    "        return response\n",
    "\n",
    "    return llm_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41172be6-750e-470d-a0d3-9a5b8b279d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/ReportsDATASET.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cca0e91-55f0-4aad-a8f8-35f73af3b108",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_radiology_report(report: str) -> str:\n",
    "    \"\"\"\n",
    "    Preprocesses a radiology report by removing unnecessary whitespace,\n",
    "    newline characters, and potential HTML tags.\n",
    "\n",
    "    Args:\n",
    "    report (str): The original radiology report text\n",
    "\n",
    "    Returns:\n",
    "    str: The preprocessed radiology report text\n",
    "    \"\"\"\n",
    "    # Remove any HTML tags\n",
    "    report = re.sub(r'<[^>]+>', '', report)\n",
    "    \n",
    "    # Replace multiple newlines and spaces with a single space\n",
    "    report = re.sub(r'\\s+', ' ', report)\n",
    "    \n",
    "    # Remove leading and trailing whitespace\n",
    "    report = report.strip()\n",
    "    \n",
    "    # Replace 'XXXX' with a placeholder like '[REDACTED]'\n",
    "    report = re.sub(r'XXXX', '[REDACTED]', report)\n",
    "    \n",
    "    return report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3b4f9d-4b86-438e-8538-687950baad36",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_report = df['Text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad21bcd4-b941-416a-b8a6-4fdc5a884f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_report = preprocess_radiology_report(example_report)\n",
    "print(preprocessed_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ee6508-9bbf-4b55-ae41-32464aa0c1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_process_abnormalities(classification_result: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Takes the JSON string output from classify_abnormalities and returns a list of\n",
    "    abnormalities that are present (have a value of 1).\n",
    "\n",
    "    Args:\n",
    "    classification_result (str): JSON string output from classify_abnormalities\n",
    "\n",
    "    Returns:\n",
    "    List[str]: List of abnormalities that are present\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Parse the JSON string into a dictionary\n",
    "        result_dict = json.loads(classification_result)\n",
    "        \n",
    "        # Filter the dictionary for keys with value 1\n",
    "        present_abnormalities = [abnormality for abnormality, value in result_dict.items() if value == 1]\n",
    "        \n",
    "        return present_abnormalities\n",
    "    except json.JSONDecodeError:\n",
    "        raise ValueError(\"Invalid JSON string provided\")\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Error processing classification result: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5038f359-0429-4c2c-9b24-6d92b6d3276f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_process_llm_output(output: str) -> str:\n",
    "    \"\"\"\n",
    "    Clean up the LLM output by removing code block markers and newlines.\n",
    "    \"\"\"\n",
    "    # Remove code block markers\n",
    "    output = re.sub(r'```(?:json)?\\s*', '', output)\n",
    "    output = output.replace('`', '')\n",
    "    \n",
    "    # Remove newlines\n",
    "    output = output.replace('\\n', ' ')\n",
    "    \n",
    "    # Remove any leading/trailing whitespace\n",
    "    output = output.strip()\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980efb98-5103-438b-8018-ce66a676c5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_abnormalities(abnormalities: List[str], report: str, llm_function) -> str:\n",
    "    # Preprocess the report\n",
    "    preprocessed_report = preprocess_radiology_report(report)\n",
    "\n",
    "    # Create a dynamic prompt for the LLM\n",
    "    prompt = f\"\"\"\n",
    "Given the following radiology report, classify the presence (1) or absence (0) of the specified abnormalities.\n",
    "Output the result as a JSON string without any additional explanation.\n",
    "\n",
    "Abnormalities to classify: {', '.join(abnormalities)}\n",
    "\n",
    "Radiology report:\n",
    "{preprocessed_report}\n",
    "\n",
    "Output format:\n",
    "{{\n",
    "    \"abnormality1\": 0 or 1,\n",
    "    \"abnormality2\": 0 or 1,\n",
    "    ...\n",
    "}}\n",
    "Return a JSON string without any explanation.\n",
    "\"\"\"\n",
    "\n",
    "    # Call the LLM function with the prompt\n",
    "    llm_output = llm_function(prompt)\n",
    "\n",
    "    # Post-process the LLM output\n",
    "    llm_output = post_process_llm_output(llm_output)\n",
    "\n",
    "    # Ensure the output is valid JSON\n",
    "    try:\n",
    "        result = json.loads(llm_output)\n",
    "        # Verify that all abnormalities are present in the output\n",
    "        for abnormality in abnormalities:\n",
    "            if abnormality not in result:\n",
    "                raise ValueError(f\"Missing abnormality in LLM output: {abnormality}\")\n",
    "        return json.dumps(result)\n",
    "    except json.JSONDecodeError:\n",
    "        raise ValueError(\"LLM output is not valid JSON\")\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Error processing LLM output: {str(e)}\")\n",
    "\n",
    "# Example usage \n",
    "def mock_llm_function(prompt: str) -> str:\n",
    "    # This is a mock function that simulates an LLM's response\n",
    "    return '{\"pulmonary edema\": 1, \"consolidation\": 0, \"pleural effusion\": 1, \"pneumothorax\": 0, \"cardiomegaly\": 1}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0657c4-8351-495d-bc02-2466026211da",
   "metadata": {},
   "outputs": [],
   "source": [
    "abnormalities = [\"pulmonary edema\", \"consolidation\", \"pleural effusion\", \"pneumothorax\", \"cardiomegaly\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0e8639-159d-46ae-8007-6cada3a12fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "result = classify_abnormalities(abnormalities, example_report, mock_llm_function)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e952603-03cd-41ac-b13f-d0b72e05ac6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the post-processing function\n",
    "present_abnormalities = post_process_abnormalities(result)\n",
    "print(\"Present abnormalities:\", present_abnormalities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "600cdc29-07ea-44c9-9b88-dc1e12134af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Create the LLM function\n",
    "llm_function = create_llm_function(original_model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206902e1-184d-4d35-83d0-e9ee6d2e6216",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_2 = df['Text'][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ab272a-7531-416a-bbcc-da173bba8b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72fdb6ab-7ede-4d1c-8118-6a3f604cf098",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = classify_abnormalities(abnormalities, example_report, llm_function)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e511249-e385-4c43-b391-958231638566",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(example_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c02f235-0a30-41bf-a1de-ccad242e8ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = classify_abnormalities(abnormalities, example_2, llm_function)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2fcf90c-3cb6-4d0a-9e46-31b016f7add1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
