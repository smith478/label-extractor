{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f292b0e8-45d5-4858-932c-cbd433351910",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db01627a-bfac-4ba2-84ee-a30481336d3c",
   "metadata": {},
   "source": [
    "## Download or load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29cacbc1-9128-48d6-9c6e-9ef3fef83f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a central location for storing models\n",
    "CENTRAL_MODEL_DIR = os.path.expanduser('~/huggingface_models')\n",
    "\n",
    "# model_name = 'microsoft/phi-2'\n",
    "# model_name = 'microsoft/phi-1_5'\n",
    "# model_name = 'microsoft/Phi-3.5-mini-instruct'\n",
    "# model_name = 'google/gemma-2-9b'\n",
    "# model_name = 'meta-llama/Meta-Llama-3.1-8B'\n",
    "# model_name = 'meta-llama/Meta-Llama-3.1-8B-Instruct'\n",
    "model_name = 'google/gemma-2-2b-it'\n",
    "# model_name = 'google/gemma-2-9b-it'\n",
    "\n",
    "# Create the central directory if it doesn't exist\n",
    "os.makedirs(CENTRAL_MODEL_DIR, exist_ok=True)\n",
    "\n",
    "# Define the path where the model will be saved locally\n",
    "local_model_path = os.path.join(CENTRAL_MODEL_DIR, model_name.replace('/', '-'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0be91bb-7c7e-480d-b941-77f79d1ebc41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automatically detect and use GPU if available, otherwise use CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Set up the device map\n",
    "if torch.cuda.is_available():\n",
    "    device_map = \"auto\"  # This will automatically distribute the model across available GPUs\n",
    "else:\n",
    "    device_map = {\"\": device}  # Use the detected device (CPU in this case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c5e9d1e-6df6-4322-9522-c68a366a0c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the model exists locally\n",
    "if os.path.exists(local_model_path):\n",
    "    print(f\"Loading model from local path: {local_model_path}\")\n",
    "    original_model = AutoModelForCausalLM.from_pretrained(\n",
    "        local_model_path,\n",
    "        device_map=device_map,\n",
    "        # quantization_config=bnb_config,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "else:\n",
    "    print(f\"Downloading model from {model_name}\")\n",
    "    original_model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        device_map=device_map,\n",
    "        # quantization_config=bnb_config,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    # Save the model locally\n",
    "    original_model.save_pretrained(local_model_path)\n",
    "    print(f\"Model saved to {local_model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b617825-be0d-4e83-8596-b81e5dd8f791",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_llm_function(model, tokenizer, max_new_tokens=512, temperature=0.7):\n",
    "    def llm_function(prompt: str) -> str:\n",
    "        # Tokenize the input prompt\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "        # Generate the output\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                temperature=temperature,\n",
    "                do_sample=True,\n",
    "                pad_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "\n",
    "        # Decode the output\n",
    "        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "        # Extract the model's response (everything after the prompt)\n",
    "        response = generated_text[len(prompt):].strip()\n",
    "\n",
    "        return response\n",
    "\n",
    "    return llm_function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82791140-6d49-485b-94dd-b8e64bf2da3f",
   "metadata": {},
   "source": [
    "## Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ce9b7e-f485-4448-b2e4-118038cf1e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/ReportsDATASET.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5744923d-5add-4bb0-8658-6cacf1181817",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9391691c-6f2f-4f61-8bfe-75431f56b5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Text'][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a9568b-500c-4c70-9d55-2fb1ed57dec4",
   "metadata": {},
   "source": [
    "## Pre and post processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49108e9c-f09f-4586-bfcb-293741715002",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def preprocess_radiology_report(report: str) -> str:\n",
    "    \"\"\"\n",
    "    Preprocesses a radiology report by removing unnecessary whitespace,\n",
    "    newline characters, and potential HTML tags.\n",
    "\n",
    "    Args:\n",
    "    report (str): The original radiology report text\n",
    "\n",
    "    Returns:\n",
    "    str: The preprocessed radiology report text\n",
    "    \"\"\"\n",
    "    # Remove any HTML tags\n",
    "    report = re.sub(r'<[^>]+>', '', report)\n",
    "    \n",
    "    # Replace multiple newlines and spaces with a single space\n",
    "    report = re.sub(r'\\s+', ' ', report)\n",
    "    \n",
    "    # Remove leading and trailing whitespace\n",
    "    report = report.strip()\n",
    "    \n",
    "    # Replace 'XXXX' with a placeholder like '[REDACTED]'\n",
    "    report = re.sub(r'XXXX', '[REDACTED]', report)\n",
    "    \n",
    "    return report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8521bc2c-8e5e-4bfc-a34d-512bd683a0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_report = df['Text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e909fa7-a4ff-44d2-aade-8b89a1d833dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6232cf-17b9-401d-b4fc-75bd4eedab7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_report = preprocess_radiology_report(example_report)\n",
    "print(preprocessed_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ad9ecf-3252-44a3-bc85-a43c1bf46d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import Dict, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9443aa31-d4f7-48ad-899d-a62635f0ad15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_process_abnormalities(classification_result: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Takes the JSON string output from classify_abnormalities and returns a list of\n",
    "    abnormalities that are present (have a value of 1).\n",
    "\n",
    "    Args:\n",
    "    classification_result (str): JSON string output from classify_abnormalities\n",
    "\n",
    "    Returns:\n",
    "    List[str]: List of abnormalities that are present\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Parse the JSON string into a dictionary\n",
    "        result_dict = json.loads(classification_result)\n",
    "        \n",
    "        # Filter the dictionary for keys with value 1\n",
    "        present_abnormalities = [abnormality for abnormality, value in result_dict.items() if value == 1]\n",
    "        \n",
    "        return present_abnormalities\n",
    "    except json.JSONDecodeError:\n",
    "        raise ValueError(\"Invalid JSON string provided\")\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Error processing classification result: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2dca69-8281-43c3-aeed-831d8dcc8139",
   "metadata": {},
   "source": [
    "## Define the labeling prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6003e584-8d5e-41a4-9f58-265676d3062c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "376d28c3-b325-4fd4-abfe-d6669fb7ea40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_abnormalities(abnormalities: List[str], report: str, llm_function) -> str:\n",
    "    # Preprocess the report\n",
    "    preprocessed_report = preprocess_radiology_report(report)\n",
    "\n",
    "    # Create a dynamic prompt for the LLM\n",
    "    prompt = f\"\"\"\n",
    "Given the following radiology report, classify the presence (1) or absence (0) of the specified abnormalities.\n",
    "Output the result as a JSON string without any additional explanation.\n",
    "\n",
    "Abnormalities to classify: {', '.join(abnormalities)}\n",
    "\n",
    "Radiology report:\n",
    "{preprocessed_report}\n",
    "\n",
    "Output format:\n",
    "{{\n",
    "    \"abnormality1\": 0 or 1,\n",
    "    \"abnormality2\": 0 or 1,\n",
    "    ...\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "    # Call the LLM function with the prompt\n",
    "    llm_output = llm_function(prompt)\n",
    "\n",
    "    # Ensure the output is valid JSON\n",
    "    try:\n",
    "        result = json.loads(llm_output)\n",
    "        # Verify that all abnormalities are present in the output\n",
    "        for abnormality in abnormalities:\n",
    "            if abnormality not in result:\n",
    "                raise ValueError(f\"Missing abnormality in LLM output: {abnormality}\")\n",
    "        return json.dumps(result)\n",
    "    except json.JSONDecodeError:\n",
    "        raise ValueError(\"LLM output is not valid JSON\")\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Error processing LLM output: {str(e)}\")\n",
    "\n",
    "# Example usage \n",
    "def mock_llm_function(prompt: str) -> str:\n",
    "    # This is a mock function that simulates an LLM's response\n",
    "    return '{\"pulmonary edema\": 1, \"consolidation\": 0, \"pleural effusion\": 1, \"pneumothorax\": 0, \"cardiomegaly\": 1}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
<<<<<<< Updated upstream
   "id": "7fcabcc1",
=======
   "id": "d8f2c87f-5d56-42a5-a68e-9a3a223517e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "abnormalities = [\"pulmonary edema\", \"consolidation\", \"pleural effusion\", \"pneumothorax\", \"cardiomegaly\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a665fd54-2712-4366-b6ba-757160117194",
>>>>>>> Stashed changes
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "result = classify_abnormalities(abnormalities, example_report, mock_llm_function)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e885bcce-f27d-4586-9089-a89e00919729",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the post-processing function\n",
    "present_abnormalities = post_process_abnormalities(result)\n",
    "print(\"Present abnormalities:\", present_abnormalities)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d2d8f1c-7465-4baa-904f-080451bc0b97",
   "metadata": {},
   "source": [
    "## Run model inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7aa84d2-8304-45c4-8261-f5156dd331b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Create the LLM function\n",
    "llm_function = create_llm_function(model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b311bc09-4ddb-4a4f-b976-06b7ae99bf4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = classify_abnormalities(abnormalities, example_report, llm_function)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66e1490",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import Dict, List\n",
    "\n",
    "def post_process_abnormalities(classification_result: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Takes the JSON string output from classify_abnormalities and returns a list of\n",
    "    abnormalities that are present (have a value of 1).\n",
    "\n",
    "    Args:\n",
    "    classification_result (str): JSON string output from classify_abnormalities\n",
    "\n",
    "    Returns:\n",
    "    List[str]: List of abnormalities that are present\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Parse the JSON string into a dictionary\n",
    "        result_dict = json.loads(classification_result)\n",
    "        \n",
    "        # Filter the dictionary for keys with value 1\n",
    "        present_abnormalities = [abnormality for abnormality, value in result_dict.items() if value == 1]\n",
    "        \n",
    "        return present_abnormalities\n",
    "    except json.JSONDecodeError:\n",
    "        raise ValueError(\"Invalid JSON string provided\")\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Error processing classification result: {str(e)}\")\n",
    "\n",
    "# Example usage\n",
    "classification_result = '{\"pulmonary edema\": 1, \"consolidation\": 0, \"pleural effusion\": 1, \"pneumothorax\": 0, \"cardiomegaly\": 1}'\n",
    "present_abnormalities = post_process_abnormalities(classification_result)\n",
    "print(present_abnormalities)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
